{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 목적\n",
    "Surrogate Model(single-objective) 구현 및  \n",
    "Surrogate Model(LightGBM, CatBoost, TabPFN)으로 multi-objective 구현하기 위해, end-to-end로 잘 동작하는 지 확인 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Model(LightGBM)으로 multi-objective 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.datasets as datasets\n",
    "import src.surrogate as surrogate\n",
    "import src.search as search\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fireducks.pandas as pd\n",
    "df = pd.read_csv('./data/concrete_processed.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['strength', 'cement', 'water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=target_cols)\n",
    "y = df[target_cols]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    "\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(X_train.shape, type(X_train))\n",
    "print(y_train.shape, type(y_train))\n",
    "print(X_test.shape, type(X_test))\n",
    "print(y_test.shape, type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_multi_train(X_train: np.ndarray,\n",
    "                         y_train: np.ndarray, \n",
    "                         params: dict = None):\n",
    "    \n",
    "    if params is None:\n",
    "        params = {\n",
    "            \"objective\": \"regression\",   \n",
    "            \"boosting_type\": \"gbdt\",     \n",
    "            \"learning_rate\": 0.05,       \n",
    "            \"num_leaves\": 31,\n",
    "            \"max_depth\": -1,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "    base_estimator = LGBMRegressor(**params, n_estimators=1000)\n",
    "    multi_model = MultiOutputRegressor(base_estimator)\n",
    "    multi_model.fit(X_train, y_train,\n",
    "                    # eval_set=[(X_val, y_val)],\n",
    "                    # eval_metric=\"rmse\",\n",
    "                    # callbacks=[\n",
    "                    # early_stopping(stopping_rounds=50),\n",
    "                    # log_evaluation(period=100)\n",
    "                    # ]\n",
    "                    )\n",
    "\n",
    "    return multi_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lightgbm_multi_train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "import joblib\n",
    "\n",
    "def lightgbm_save(model, path):\n",
    "    \"\"\"MultiOutputRegressor의 개별 LightGBM 모델을 저장\"\"\"\n",
    "    # models = model.estimators_  # 내부의 개별 모델 리스트 가져오기\n",
    "    # joblib.dump(models, path)\n",
    "    joblib.dump(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_save(model, './model_save/multi_lightgbm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_load(path):\n",
    "    \"\"\"joblib을 이용해 MultiOutputRegressor를 불러오기\"\"\"\n",
    "    return joblib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = lightgbm_load('./model_save/multi_lightgbm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lightgbm_multi_evaluate(model, y_train, y_pred, y_test, target_cols):\n",
    "    rmse_list, mae_list, r2_list = [], [], []\n",
    "    \n",
    "    for idx, col in enumerate(target_cols):\n",
    "        y_true_i = y_test[:, idx]  # target_dict의 key를 인덱스로 변환\n",
    "        y_pred_i = y_pred[:, idx]\n",
    "        \n",
    "        mae_i = np.mean(np.abs(y_true_i - y_pred_i))\n",
    "        mse_i = np.mean((y_true_i - y_pred_i) ** 2)\n",
    "        rmse_i = np.sqrt(mse_i)\n",
    "        \n",
    "        sse_i = np.sum((y_true_i - y_pred_i) ** 2)\n",
    "        sst_i = np.sum(y_true_i - np.mean(y_train) ** 2)\n",
    "        \n",
    "        r2_i = 1 - sse_i / sst_i\n",
    "        \n",
    "        rmse_list.append(rmse_i)\n",
    "        mae_list.append(mae_i)\n",
    "        r2_list.append(r2_i)\n",
    "        \n",
    "        print(f\"Target '{col}' - RMSE: {rmse_i:.4f}, MAE: {mae_i:.4f}, R2: {r2_i:.4f}\")\n",
    "    \n",
    "    rmse_mean = np.mean(rmse_list)\n",
    "    mae_mean = np.mean(mae_list)\n",
    "    r2_mean = np.mean(r2_list)\n",
    "    \n",
    "    print(f\"[Average Metrics] RMSE: {rmse_mean:.4f}, MAE: {mae_mean:.4f}, R2: {r2_mean:.4f}\")\n",
    "    return rmse_mean, mae_mean, r2_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_mean, mae_mean, r2_mean = lightgbm_multi_evaluate(model,\n",
    "                        y_train, \n",
    "                        y_pred,\n",
    "                        y_test,\n",
    "                        target_cols\n",
    "                        )\n",
    "\n",
    "print(f'lightGBM 모델 mulit-objective RMSE: {rmse_mean:.4f}')\n",
    "print(f'lightGBM 모델 mulit-objective MAE: {mae_mean:.4f}')\n",
    "print(f'lightGBM 모델 mulit-objective R^2: {r2_mean:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Model(CatBoost)으로 single-objective 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.datasets as datasets\n",
    "import src.surrogate as surrogate\n",
    "import src.search as search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_func = getattr(datasets, f'cement_data')\n",
    "X_train, X_test, y_train, y_test = load_data_func('./data/concrete_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교를 위한 선형회귀\n",
    "from sklearn.linear_model import LinearRegression\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "baseline_pred = baseline_model.predict(X_test)\n",
    "baseline_r2 = 1 - np.sum((y_test - baseline_pred) ** 2) / (np.sum((y_test - y_train.mean()) ** 2) + 1e-10)\n",
    "print(f'선형회귀 모델 R^2: {baseline_r2.item() :.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(\n",
    "    iterations=2000,        # 학습 반복 횟수\n",
    "    depth=7,                # 트리 깊이\n",
    "    learning_rate=0.05,     # 학습률\n",
    "    bagging_temperature=1, # 앙상블 다양성을 조절 (1~3 추천)\n",
    "    # l2_leaf_reg=5,         # L2 정규화 (3~10 사이에서 튜닝 가능)\n",
    "    loss_function='RMSE',   # 손실 함수 (회귀 문제이므로 RMSE 사용)\n",
    "    # eval_metric='RMSE',     # 평가 지표\n",
    "    random_seed=42,\n",
    "    verbose=100,            # 학습 과정 출력\n",
    "    early_stopping_rounds=100  # 조기 종료\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_surrogate_model(y_train,y_pred, y_test):\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    SSE = np.sum(np.square(y_test - y_pred))    \n",
    "    SST = np.sum(np.square(y_test - y_train.mean()))\n",
    "    r2 = 1 - SSE/SST\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_pred.shape)\n",
    "if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse, mae, r2 = eval_surrogate_model(y_train, y_pred, y_test)\n",
    "print(f'catboost 모델 RMSE: {rmse:.4f}')\n",
    "print(f'catboost 모델 MAE: {mae:.4f}')\n",
    "print(f'catboost 모델 R^2: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델저장\n",
    "def catboost_save(model, path):\n",
    "    \"\"\"CatBoost 모델 저장\"\"\"\n",
    "    model.save_model(path, format=\"cbm\")  # CatBoost 전용 포맷으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_save(model,'./model_save/catboost_model.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델불러오기\n",
    "def catboost_load(path):\n",
    "    \"\"\"CatBoost 모델 불러오기\"\"\"\n",
    "    model = CatBoostRegressor()  # 회귀 모델이면 CatBoostRegressor, 분류 모델이면 CatBoostClassifier\n",
    "    model.load_model(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = catboost_load('./model_save/catboost_model.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Model(CatBoost)으로 multi-objective 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fireducks.pandas as pd\n",
    "df = pd.read_csv('./data/concrete_processed.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['strength', 'cement', 'water']\n",
    "X = df.drop(columns=target_cols)\n",
    "y = df[target_cols]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, type(X_train))\n",
    "print(y_train.shape, type(y_train))\n",
    "print(X_test.shape, type(X_test))\n",
    "print(y_test.shape, type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(X_train.shape, type(X_train))\n",
    "print(y_train.shape, type(y_train))\n",
    "print(X_test.shape, type(X_test))\n",
    "print(y_test.shape, type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법 1 : 개별모델학습해서 multi-objective 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(y.shape[1]):\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=2000,\n",
    "        depth=7,\n",
    "        learning_rate=0.05,\n",
    "        loss_function='RMSE',\n",
    "        random_seed=42,\n",
    "        verbose=200\n",
    "    )\n",
    "    model.fit(X_train, y_train[:, i], early_stopping_rounds=100)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "for m in models:\n",
    "    y_pred = m.predict(X_test)\n",
    "    print(y_pred.shape)\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "    print(y_pred.shape)\n",
    "    y_preds.append(y_pred)\n",
    "y_preds = np.column_stack(y_preds)\n",
    "print(y_preds.shape)\n",
    "\n",
    "# y_preds = np.column_stack([m.predict(X_test) for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def catboost_multi_evaluate(model, y_train, y_pred, y_test, target_cols):\n",
    "    rmse_list, mae_list, r2_list = [], [], []\n",
    "    \n",
    "    for idx, col in enumerate(target_cols):\n",
    "        y_true_i = y_test[:, idx]  # target_dict의 key를 인덱스로 변환\n",
    "        y_pred_i = y_pred[:, idx]\n",
    "        \n",
    "        mae_i = np.mean(np.abs(y_true_i - y_pred_i))\n",
    "        mse_i = np.mean((y_true_i - y_pred_i) ** 2)\n",
    "        rmse_i = np.sqrt(mse_i)\n",
    "        \n",
    "        sse_i = np.sum((y_true_i - y_pred_i) ** 2)\n",
    "        sst_i = np.sum(y_true_i - np.mean(y_train) ** 2)\n",
    "        \n",
    "        r2_i = 1 - sse_i / sst_i\n",
    "        \n",
    "        rmse_list.append(rmse_i)\n",
    "        mae_list.append(mae_i)\n",
    "        r2_list.append(r2_i)\n",
    "        \n",
    "        print(f\"Target '{col}' - RMSE: {rmse_i:.4f}, MAE: {mae_i:.4f}, R2: {r2_i:.4f}\")\n",
    "    \n",
    "    rmse_mean = np.mean(rmse_list)\n",
    "    mae_mean = np.mean(mae_list)\n",
    "    r2_mean = np.mean(r2_list)\n",
    "    \n",
    "    print(f\"[Average Metrics] RMSE: {rmse_mean:.4f}, MAE: {mae_mean:.4f}, R2: {r2_mean:.4f}\")\n",
    "    return rmse_mean, mae_mean, r2_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_mean, mae_mean, r2_mean = catboost_multi_evaluate(model,\n",
    "                            y_train,    \n",
    "                            y_preds,  \n",
    "                            y_test,   \n",
    "                            target_cols)\n",
    "print(f'catboost 모델 mulit-objective RMSE: {rmse_mean:.4f}')\n",
    "print(f'catboost 모델 mulit-objective MAE: {mae_mean:.4f}')\n",
    "print(f'catboost 모델 mulit-objective R^2: {r2_mean:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법 2 : custom loss function 활용 - 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiObjectiveLoss:\n",
    "    def __init__(self, alpha=0.5, beta=0.5):\n",
    "        \"\"\"\n",
    "        Multi-Objective 손실 함수 (RMSE + MAE)\n",
    "        alpha: RMSE 가중치\n",
    "        beta: MAE 가중치\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "    \n",
    "    def calc_ders_range(self, approxes, targets, weights):\n",
    "        \"\"\"\n",
    "        approxes: 예측값 (logits)\n",
    "        targets: 실제 값\n",
    "        weights: 샘플 가중치 (필요 없을 경우 None)\n",
    "        \"\"\"\n",
    "        assert len(approxes) == len(targets)\n",
    "        \n",
    "        derivatives = []\n",
    "        second_derivatives = []\n",
    "        \n",
    "        for i in range(len(approxes)):\n",
    "            error1 = approxes[i] - targets[i]  # RMSE 오차\n",
    "            error2 = approxes[i] - targets[i]  # MAE 오차\n",
    "            \n",
    "            grad_rmse = error1 # RMSE 미분값\n",
    "            grad_mae = np.sign(error2) # MAE 미분값\n",
    "            \n",
    "            grad = self.alpha * grad_rmse + self.beta * grad_mae  # Gradient (1차 미분)\n",
    "            # Hessian (2차 미분) - 보통 1로 설정\n",
    "            hess = 1.0  # CatBoost에서는 Hessian을 안 쓰는 경우가 많음\n",
    "            \n",
    "            derivatives.append(grad)\n",
    "            second_derivatives.append(hess)\n",
    "        \n",
    "        return zip(derivatives, second_derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    depth=7,\n",
    "    learning_rate=0.05,\n",
    "    loss_function=MultiObjectiveLoss(alpha=0.7, beta=0.3),  # 가중치 조정 가능\n",
    "    random_seed=42,\n",
    "    verbose=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법 3 :  CatBoost의 MultiRegression 모드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    depth=7,\n",
    "    learning_rate=0.05,\n",
    "    loss_function=\"MultiRMSE\",  # MultiRegression을 위한 손실 함수\n",
    "    random_seed=42,\n",
    "    verbose=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_test)\n",
    "print(y_preds.shape)\n",
    "print(type(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape)\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def catboost_multi_evaluate(model, y_train, y_pred, y_test, target_cols):\n",
    "    rmse_list, mae_list, r2_list = [], [], []\n",
    "    \n",
    "    for idx, col in enumerate(target_cols):\n",
    "        y_true_i = y_test[:, idx]  # target_dict의 key를 인덱스로 변환\n",
    "        y_pred_i = y_pred[:, idx]\n",
    "        \n",
    "        mae_i = np.mean(np.abs(y_true_i - y_pred_i))\n",
    "        mse_i = np.mean((y_true_i - y_pred_i) ** 2)\n",
    "        rmse_i = np.sqrt(mse_i)\n",
    "        \n",
    "        sse_i = np.sum((y_true_i - y_pred_i) ** 2)\n",
    "        sst_i = np.sum(y_true_i - np.mean(y_train) ** 2)\n",
    "        \n",
    "        r2_i = 1 - sse_i / sst_i\n",
    "        \n",
    "        rmse_list.append(rmse_i)\n",
    "        mae_list.append(mae_i)\n",
    "        r2_list.append(r2_i)\n",
    "        \n",
    "        print(f\"Target '{col}' - RMSE: {rmse_i:.4f}, MAE: {mae_i:.4f}, R2: {r2_i:.4f}\")\n",
    "    \n",
    "    rmse_mean = np.mean(rmse_list)\n",
    "    mae_mean = np.mean(mae_list)\n",
    "    r2_mean = np.mean(r2_list)\n",
    "    \n",
    "    print(f\"[Average Metrics] RMSE: {rmse_mean:.4f}, MAE: {mae_mean:.4f}, R2: {r2_mean:.4f}\")\n",
    "    return rmse_mean, mae_mean, r2_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_mean, mae_mean, r2_mean = catboost_multi_evaluate(model,\n",
    "                            y_train,    \n",
    "                            y_preds,  \n",
    "                            y_test,   \n",
    "                            target_cols)\n",
    "print(f'catboost 모델 mulit-objective RMSE: {rmse_mean:.4f}')\n",
    "print(f'catboost 모델 mulit-objective MAE: {mae_mean:.4f}')\n",
    "print(f'catboost 모델 mulit-objective R^2: {r2_mean:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델저장\n",
    "def catboost_save(model, path):\n",
    "    \"\"\"\n",
    "    CatBoost 모델을 지정된 경로에 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        model (CatBoostRegressor): 저장할 CatBoost 모델 객체\n",
    "        path (str): 모델을 저장할 파일 경로\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.save_model(path, format='cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_save(model,'./model_save/catboost_multi_model.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델불러오기\n",
    "def catboost_load(path):\n",
    "    \"\"\"CatBoost 모델 불러오기\"\"\"\n",
    "    model = CatBoostRegressor()  # 회귀 모델이면 CatBoostRegressor, 분류 모델이면 CatBoostClassifier\n",
    "    model.load_model(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = catboost_load('./model_save/catboost_multi_model.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Model(TabPFN)으로 multi-objective 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn import TabPFNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(y_train.shape[1]):\n",
    "    model = TabPFNRegressor()\n",
    "    # model.fit(X_train, y_train.iloc[:, i])\n",
    "    model.fit(X_train, y_train[:, i])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "for m in models:\n",
    "    y_pred = m.predict(X_test)\n",
    "    print(y_pred.shape)\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "    print(y_pred.shape)\n",
    "    y_preds.append(y_pred)\n",
    "y_preds = np.column_stack(y_preds)\n",
    "print(y_preds.shape, type(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tabpfn_multi_evaluate(model, y_train, y_pred, y_test, target_cols):\n",
    "    rmse_list, mae_list, r2_list = [], [], []\n",
    "    \n",
    "    for idx, col in enumerate(target_cols):\n",
    "        y_true_i = y_test[:, idx]  # target_dict의 key를 인덱스로 변환\n",
    "        y_pred_i = y_pred[:, idx]\n",
    "        \n",
    "        mae_i = np.mean(np.abs(y_true_i - y_pred_i))\n",
    "        mse_i = np.mean((y_true_i - y_pred_i) ** 2)\n",
    "        rmse_i = np.sqrt(mse_i)\n",
    "        \n",
    "        sse_i = np.sum((y_true_i - y_pred_i) ** 2)\n",
    "        sst_i = np.sum(y_true_i - np.mean(y_train) ** 2)\n",
    "        \n",
    "        r2_i = 1 - sse_i / sst_i\n",
    "        \n",
    "        rmse_list.append(rmse_i)\n",
    "        mae_list.append(mae_i)\n",
    "        r2_list.append(r2_i)\n",
    "        \n",
    "        print(f\"Target '{col}' - RMSE: {rmse_i:.4f}, MAE: {mae_i:.4f}, R2: {r2_i:.4f}\")\n",
    "    \n",
    "    rmse_mean = np.mean(rmse_list)\n",
    "    mae_mean = np.mean(mae_list)\n",
    "    r2_mean = np.mean(r2_list)\n",
    "    \n",
    "    print(f\"[Average Metrics] RMSE: {rmse_mean:.4f}, MAE: {mae_mean:.4f}, R2: {r2_mean:.4f}\")\n",
    "    return rmse_mean, mae_mean, r2_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_mean, mae_mean, r2_mean = tabpfn_multi_evaluate(model,\n",
    "                            y_train,    \n",
    "                            y_preds,  \n",
    "                            y_test,   \n",
    "                            target_cols)\n",
    "print(f'tabpfn 모델 mulit-objective RMSE: {rmse_mean:.4f}')\n",
    "print(f'tabpfn 모델 mulit-objective MAE: {mae_mean:.4f}')\n",
    "print(f'tabpfn 모델 mulit-objective R^2: {r2_mean:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
